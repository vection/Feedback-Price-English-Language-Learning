{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Prize Best score 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  \\\n",
       "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0   \n",
       "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0   \n",
       "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0   \n",
       "\n",
       "   conventions  \n",
       "0          3.0  \n",
       "1          3.0  \n",
       "2          3.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_types)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class GrammerDataset(Dataset):\n",
    "    pos_tag_vocab = ['CC',\n",
    "            'WRB',\n",
    "            'EX',\n",
    "            'MD',\n",
    "            'VBN',\n",
    "            'VBD',\n",
    "            'NNS',\n",
    "            'RBR',\n",
    "            'VBZ',\n",
    "            'PRP$',\n",
    "            'VB',\n",
    "            'RP',\n",
    "            'WP',\n",
    "            'VBP',\n",
    "            'JJR',\n",
    "            'VBG',\n",
    "            'PDT',\n",
    "            'JJ',\n",
    "            'JJS',\n",
    "            'WDT',\n",
    "            'IN',\n",
    "            'DT',\n",
    "            'RB',\n",
    "            'NN',\n",
    "            'PRP',\n",
    "            'TO']\n",
    "\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Dataset object for base model\n",
    "        :param data:\n",
    "        '''\n",
    "        self.text,self.text_length,self.pos_tag = self.clean_text(data['full_text'])\n",
    "        self.cohesion = np.array(self.feature_transformation(data['cohesion']))\n",
    "        self.syntax = np.array(self.feature_transformation(data['syntax']))\n",
    "        self.vocab = np.array(self.feature_transformation((data['vocabulary'])))\n",
    "        self.phraseology = np.array(self.feature_transformation(data['phraseology']))\n",
    "        self.grammer = np.array(self.feature_transformation(data['grammar']))\n",
    "        self.conventions = np.array(self.feature_transformation(data['conventions']))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def feature_transformation(self, vals):\n",
    "        self.mapping = {1:0,1.5:1,2:2,2.5:3,3:4,3.5:5,4:6,4.5:7,5:8}\n",
    "        newl = []\n",
    "        for v in vals:\n",
    "            newl.append(self.mapping[v])\n",
    "        return newl\n",
    "    def count_pos_tag(self,text):\n",
    "        tag_dict = {}\n",
    "        pos = nltk.pos_tag(text)\n",
    "        tag_types = list(set([item[1] for item in pos]))\n",
    "        for tag in tag_types:\n",
    "            tag_dict[tag] = sum([item[1]==tag for item in pos])\n",
    "        return tag_dict\n",
    "    \n",
    "\n",
    "    def clean_text(self,all_text):\n",
    "        newl = []\n",
    "        newlength = []\n",
    "        pos_tag = []\n",
    "        for text in all_text:\n",
    "            text = text.replace(\"\\n\",\" \").lower()\n",
    "            text = text.strip()\n",
    "            newl.append(text)\n",
    "            newlength.append([len(t) for t in text.split()])\n",
    "            pos_sentence = self.count_pos_tag(text.split())\n",
    "            hot_vector_pos = []\n",
    "            for po in self.pos_tag_vocab:\n",
    "                if po in pos_sentence.keys():\n",
    "                    hot_vector_pos.append(pos_sentence[po])\n",
    "                else:\n",
    "                    hot_vector_pos.append(0)\n",
    "            pos_tag.append(hot_vector_pos)\n",
    "            \n",
    "            \n",
    "        return np.array(newl),np.array(newlength),np.array(pos_tag)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx],self.text_length[idx],self.pos_tag[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "train=df.sample(frac=0.95,random_state=155) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "train_dataset = GrammerDataset(train)\n",
    "valid_dataset = GrammerDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import(\n",
    "    get_tokenizer,\n",
    "    ngrams_iterator,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "ngrams = 2\n",
    "def yield_tokens(data_iter, ngrams):\n",
    "    for text in data_iter:\n",
    "        yield iter(tokenizer(text))\n",
    "\n",
    "myvocab = build_vocab_from_iterator(yield_tokens(train_dataset.text, ngrams), specials=[\"<unk>\"])\n",
    "myvocab.set_default_index(myvocab[\"<unk>\"])\n",
    "device = 'cuda'\n",
    "def text_pipeline(x,padding=1024): \n",
    "    res = myvocab(list((tokenizer(x))))\n",
    "    if len(res) < padding:\n",
    "        chars_to_add = int(padding-len(res))\n",
    "        for i in range(chars_to_add):\n",
    "            res.append(myvocab['<pad>'])\n",
    "    else:\n",
    "        res = res[:padding]\n",
    "    return res\n",
    "# self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "def collate_batch(batch):\n",
    "    text_list, cohesion, syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = [], [],[],[],[],[],[],[],[]\n",
    "    for data in batch:\n",
    "        processed_text = text_pipeline(data[0],1024)\n",
    "        text_list.append(processed_text)\n",
    "        text_length.append(len(processed_text))\n",
    "        cohesion.append(data[1])\n",
    "        syntax.append(data[2])\n",
    "        vocab.append(data[3])\n",
    "        phraseology.append(data[4])\n",
    "        grammer.append(data[5])\n",
    "        conventions.append(data[6])\n",
    "        pos_tag.append(data[8])\n",
    "        \n",
    "        \n",
    "    cohesion = torch.tensor(cohesion, dtype=torch.int64)\n",
    "    syntax = torch.tensor(syntax, dtype=torch.int64)\n",
    "    vocab = torch.tensor(vocab, dtype=torch.int64)\n",
    "    phraseology = torch.tensor(phraseology, dtype=torch.int64)\n",
    "    grammer = torch.tensor(grammer, dtype=torch.int64)\n",
    "    conventions = torch.tensor(conventions, dtype=torch.int64)\n",
    "    pos_tag_vals = torch.tensor(pos_tag, dtype=torch.int64)\n",
    "    text_length = torch.tensor(text_length, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    return text_list.to(device), cohesion.to(device), syntax.to(device),vocab.to(device), phraseology.to(device),grammer.to(device), conventions.to(device),text_length.to(device),pos_tag_vals.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self, num_scored=3):\n",
    "        super().__init__()\n",
    "        self.rmse = RMSELoss()\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score += self.rmse(yhat[:, :, i], y[:, :, i]) / self.num_scored\n",
    "\n",
    "        return score\n",
    "    \n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout=0.3, pad_idx=0,\n",
    "                 fc_hidden2=64, fc_hidden1=256):\n",
    "        super().__init__()\n",
    "        self.hidden_size = fc_hidden2\n",
    "        self.input_dim = input_dim\n",
    "        self.bid = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout \n",
    "        self.fc_hidden = fc_hidden1\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # embedded to cuda if available\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "\n",
    "        #tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "        #model = DebertaModel.from_pretrained('microsoft/deberta-base', return_dict=True)\n",
    "\n",
    "#         inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "#         outputs = model(**inputs,return_dict=True)\n",
    "\n",
    "        self.num_neurons = (hidden_dim * num_layers) + 26 \n",
    "        \n",
    "        self.labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "        self.featurs_nn = []\n",
    "        for lab in self.labels:\n",
    "            fc = [nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "                  nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),nn.Linear(self.hidden_size, output_dim)]\n",
    "            fc = nn.Sequential(*fc)\n",
    "            self.featurs_nn.append(fc)\n",
    "            \n",
    "        self.featurs_nn = nn.Sequential(*self.featurs_nn)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text,text_length,pos_tag):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        text_length = torch.clamp(text_length, min=1)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length.cpu(),batch_first=True,enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        res = torch.cat((hidden,pos_tag),dim=1)\n",
    "        output_dict = {}\n",
    "        for f_nn,feature in zip(self.featurs_nn,self.labels):\n",
    "            newh = res.clone()\n",
    "            out = f_nn(newh)\n",
    "            output_dict[feature] = out\n",
    "        #hidden = self.fc(hidden)\n",
    "        return output_dict\n",
    "\n",
    "def save_model(model, path):\n",
    "    global myvocab\n",
    "    cfg = model.state_dict()\n",
    "    cfg['input_dim'] = model.input_dim\n",
    "    cfg['embedding_dim'] = model.embedding_dim\n",
    "    cfg['bidirectional'] = model.bid\n",
    "    cfg['hidden_dim'] = model.hidden_dim\n",
    "    cfg['num_layers'] = model.num_layers\n",
    "    cfg['output_dim'] = model.output_dim\n",
    "    cfg['dropout'] = model.dropout_rate\n",
    "    torch.save(cfg, path)\n",
    "    torch.save(myvocab,path.replace(\".pth\",\"\")+'_vocab.pth')\n",
    "    \n",
    "def load_model(path):\n",
    "    keys_to_remove = ['input_dim', 'embedding_dim', 'bidirectional', 'hidden_dim', 'num_layers', 'output_dim','dropout']\n",
    "    cfg = torch.load(path)\n",
    "    model = RNN_CNN(cfg['input_dim'],cfg['embedding_dim'],cfg['bidirectional'],cfg['hidden_dim'],cfg['num_layers'],cfg['output_dim'],cfg['dropout'])\n",
    "    \n",
    "    for key in keys_to_remove:\n",
    "        cfg.pop(key)\n",
    "    model.load_state_dict(cfg)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_vocab(path):\n",
    "    vocab = torch.load(path)\n",
    "    return vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(21426, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (featurs_nn): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=538, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Linear(in_features=64, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(myvocab)\n",
    "batch_size = 64\n",
    "bidirectional = True\n",
    "emb_dim = 300\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch)\n",
    "\n",
    "model = RNN(vocab_size, emb_dim,bidirectional,hidden_dim=256,num_layers=2,output_dim=9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(y_vals):\n",
    "    mapping = {0: 1, 1: 1.5, 2: 2, 3: 2.5, 4: 3, 5: 3.5, 6: 4, 7: 4.5, 8: 5}\n",
    "    if isinstance(y_vals, torch.Tensor):\n",
    "        if len(y_vals) > 0:\n",
    "            y_vals = y_vals.tolist()\n",
    "            newl = []\n",
    "            for i in y_vals:\n",
    "                newl.append(mapping[i])\n",
    "            return newl\n",
    "        return mapping[y_vals.item()]\n",
    "    elif isinstance(y_vals,int):\n",
    "         return mapping[y_vals]\n",
    "    else:\n",
    "        if len(y_vals) > 0:\n",
    "            y_vals = y_vals.tolist()\n",
    "            newl = []\n",
    "            for i in y_vals:\n",
    "                newl.append(mapping[i])\n",
    "            return newl\n",
    "def compute_acc(predicted,gt):\n",
    "    total_true = sum([predicted[i]==gt[i] for i in range(len(gt))])\n",
    "    return total_true/len(gt), total_true\n",
    "def compare_tensor_acc(preds, gt):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    proba, class_label = preds.max(dim=1)\n",
    "    predict_total = [(transform_labels(cl), pr) for cl,pr in zip(class_label.tolist(),proba.tolist())]\n",
    "    predict_only = [pt[0] for pt in predict_total]\n",
    "    gra_acc,true = compute_acc(predict_only,gt.tolist())\n",
    "    return predict_only,true\n",
    "\n",
    "import math\n",
    "def mcrmse_score(labels,y_size=6):\n",
    "    #labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "    multiplier = (1/y_size)\n",
    "    loss_sum = 0\n",
    "    i = 0\n",
    "    for item in labels:\n",
    "        pred_vector = item[0]\n",
    "        gt_vector = item[1]\n",
    "        loss_sum += math.sqrt((1/len(pred_vector)) * sum([(pred_vector[i]-gt_vector[i])**2 for i in range(len(pred_vector))]))\n",
    "    \n",
    "    return loss_sum*multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 13.35218620300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352464346/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model saved! better average RMCMSE score  1.7371624703125117\n",
      "Average loss:  10.470125667119431\n",
      "MCRmse average score:  1.7371624703125117\n",
      "Training accuracy!\n",
      "Cohesion:  0.10901749663526245  Syntax:  0.14212651413189772  Vocab:  0.12839838492597577\n",
      "Phraseology:  0.12355316285329744  Grammar:  0.1028263795423957  Conventions:  0.1440107671601615\n",
      "Valdiation Avg Score:  6.935617606861866\n",
      "Epoch 1\n",
      "Loss: 10.363153457641602\n",
      "New model saved! better average RMCMSE score  1.6920933347104405\n",
      "Valdiation Avg Score:  6.817420096488002\n",
      "Epoch 2\n",
      "Loss: 9.287731170654297\n",
      "New model saved! better average RMCMSE score  1.6675317832976049\n",
      "Valdiation Avg Score:  7.436793353216184\n",
      "Epoch 3\n",
      "Loss: 9.721858978271484\n",
      "New model saved! better average RMCMSE score  1.6540832042866145\n",
      "Valdiation Avg Score:  6.6048067458980135\n",
      "Epoch 4\n",
      "Loss: 9.190184593200684\n",
      "New model saved! better average RMCMSE score  1.6482430332750444\n",
      "Valdiation Avg Score:  6.040112520647576\n",
      "Epoch 5\n",
      "Loss: 8.380167961120605\n",
      "New model saved! better average RMCMSE score  1.6102425828114197\n",
      "Valdiation Avg Score:  6.428514666133326\n",
      "Epoch 6\n",
      "Loss: 8.819323539733887\n",
      "New model saved! better average RMCMSE score  1.6035357864939046\n",
      "Valdiation Avg Score:  6.72395895199141\n",
      "Epoch 7\n",
      "Loss: 8.460472106933594\n",
      "New model saved! better average RMCMSE score  1.563008618228276\n",
      "Valdiation Avg Score:  5.8779353516843225\n",
      "Epoch 8\n",
      "Loss: 8.349687576293945\n",
      "New model saved! better average RMCMSE score  1.52201081230049\n",
      "Valdiation Avg Score:  6.950214823351041\n",
      "Epoch 9\n",
      "Loss: 7.923083782196045\n",
      "New model saved! better average RMCMSE score  1.5194411015912102\n",
      "Valdiation Avg Score:  6.346457419919067\n",
      "Epoch 10\n",
      "Loss: 7.776366710662842\n",
      "New model saved! better average RMCMSE score  1.4919941868966753\n",
      "Average loss:  7.757010411407988\n",
      "MCRmse average score:  1.4919941868966753\n",
      "Training accuracy!\n",
      "Cohesion:  0.09582772543741588  Syntax:  0.10578734858681023  Vocab:  0.11790040376850605\n",
      "Phraseology:  0.12543741588156124  Grammar:  0.10740242261103634  Conventions:  0.12032301480484522\n",
      "Valdiation Avg Score:  5.937414598159906\n",
      "Epoch 11\n",
      "Loss: 7.637101650238037\n",
      "Valdiation Avg Score:  6.688069620950556\n",
      "Epoch 12\n",
      "Loss: 7.581475734710693\n",
      "New model saved! better average RMCMSE score  1.4915667170603593\n",
      "Valdiation Avg Score:  5.967659630846439\n",
      "Epoch 13\n",
      "Loss: 7.317215919494629\n",
      "New model saved! better average RMCMSE score  1.4798491500140625\n",
      "Valdiation Avg Score:  6.34432079718422\n",
      "Epoch 14\n",
      "Loss: 7.148747444152832\n",
      "New model saved! better average RMCMSE score  1.4651482450244944\n",
      "Valdiation Avg Score:  7.008668129975382\n",
      "Epoch 15\n",
      "Loss: 7.08308744430542\n",
      "New model saved! better average RMCMSE score  1.4560551379416264\n",
      "Valdiation Avg Score:  7.285268596379835\n",
      "Epoch 16\n",
      "Loss: 6.771395206451416\n",
      "Valdiation Avg Score:  5.891953789049044\n",
      "Epoch 17\n",
      "Loss: 6.537846088409424\n",
      "New model saved! better average RMCMSE score  1.4497871843417005\n",
      "Valdiation Avg Score:  7.052414222423858\n",
      "Epoch 18\n",
      "Loss: 6.629987716674805\n",
      "New model saved! better average RMCMSE score  1.4421573976723658\n",
      "Valdiation Avg Score:  6.120090205440511\n",
      "Epoch 19\n",
      "Loss: 7.107694149017334\n",
      "Valdiation Avg Score:  5.887423750993025\n",
      "Epoch 20\n",
      "Loss: 6.464260578155518\n",
      "New model saved! better average RMCMSE score  1.435627426298975\n",
      "Average loss:  6.799313569473008\n",
      "MCRmse average score:  1.435627426298975\n",
      "Training accuracy!\n",
      "Cohesion:  0.07913862718707941  Syntax:  0.11574697173620457  Vocab:  0.1009421265141319\n",
      "Phraseology:  0.1243606998654105  Grammar:  0.12113055181695828  Conventions:  0.12166890982503364\n",
      "Valdiation Avg Score:  7.075085231135762\n",
      "Epoch 21\n",
      "Loss: 6.407870769500732\n",
      "Valdiation Avg Score:  6.7675177149497046\n",
      "Epoch 22\n",
      "Loss: 6.294619083404541\n",
      "New model saved! better average RMCMSE score  1.4257006963371763\n",
      "Valdiation Avg Score:  6.766071882843354\n",
      "Epoch 23\n",
      "Loss: 6.658988952636719\n",
      "Valdiation Avg Score:  6.793861621804528\n",
      "Epoch 24\n",
      "Loss: 6.290918827056885\n",
      "New model saved! better average RMCMSE score  1.4160320201959118\n",
      "Valdiation Avg Score:  6.592302457764616\n",
      "Epoch 25\n",
      "Loss: 6.285374164581299\n",
      "New model saved! better average RMCMSE score  1.4134265174005582\n",
      "Valdiation Avg Score:  6.2870063189798735\n",
      "Epoch 26\n",
      "Loss: 5.836544036865234\n",
      "New model saved! better average RMCMSE score  1.40950827655179\n",
      "Valdiation Avg Score:  7.493160384526749\n",
      "Epoch 27\n",
      "Loss: 5.9863386154174805\n",
      "New model saved! better average RMCMSE score  1.4031888504172771\n",
      "Valdiation Avg Score:  6.732954385105577\n",
      "Epoch 28\n",
      "Loss: 5.855818271636963\n",
      "Valdiation Avg Score:  6.450859851838446\n",
      "Epoch 29\n",
      "Loss: 5.819965839385986\n",
      "Valdiation Avg Score:  6.318638800189592\n",
      "Epoch 30\n",
      "Loss: 5.854037284851074\n",
      "Average loss:  5.867138062493276\n",
      "MCRmse average score:  1.4042507038437342\n",
      "Training accuracy!\n",
      "Cohesion:  0.0847913862718708  Syntax:  0.12866756393001347  Vocab:  0.0864064602960969\n",
      "Phraseology:  0.12382234185733512  Grammar:  0.13189771197846567  Conventions:  0.12839838492597577\n",
      "Valdiation Avg Score:  6.5274731562607755\n",
      "Epoch 31\n",
      "Loss: 6.524966239929199\n",
      "New model saved! better average RMCMSE score  1.3940188079080393\n",
      "Valdiation Avg Score:  7.054021470781421\n",
      "Epoch 32\n",
      "Loss: 5.665154457092285\n",
      "Valdiation Avg Score:  6.510122759025629\n",
      "Epoch 33\n",
      "Loss: 5.479778289794922\n",
      "Valdiation Avg Score:  6.759552867643484\n",
      "Epoch 34\n",
      "Loss: 5.0516791343688965\n",
      "New model saved! better average RMCMSE score  1.393001357393258\n",
      "Valdiation Avg Score:  6.081363921564795\n",
      "Epoch 35\n",
      "Loss: 5.095142841339111\n",
      "Valdiation Avg Score:  7.054244770633135\n",
      "Epoch 36\n",
      "Loss: 4.92549467086792\n",
      "New model saved! better average RMCMSE score  1.37695996626697\n",
      "Valdiation Avg Score:  6.803451057348817\n",
      "Epoch 37\n",
      "Loss: 5.294826507568359\n",
      "Valdiation Avg Score:  7.529758426408842\n",
      "Epoch 38\n",
      "Loss: 4.594766139984131\n",
      "Valdiation Avg Score:  6.247102826733086\n",
      "Epoch 39\n",
      "Loss: 4.996638298034668\n",
      "Valdiation Avg Score:  5.974920743849016\n",
      "Epoch 40\n",
      "Loss: 4.938480854034424\n",
      "New model saved! better average RMCMSE score  1.3711070962579532\n",
      "Average loss:  4.905484878410728\n",
      "MCRmse average score:  1.3711070962579532\n",
      "Training accuracy!\n",
      "Cohesion:  0.08317631224764468  Syntax:  0.12462987886944818  Vocab:  0.07429340511440108\n",
      "Phraseology:  0.10901749663526245  Grammar:  0.14347240915208614  Conventions:  0.12839838492597577\n",
      "Valdiation Avg Score:  7.238428422500894\n",
      "Epoch 41\n",
      "Loss: 4.590069770812988\n",
      "Valdiation Avg Score:  7.147803954903097\n",
      "Epoch 42\n",
      "Loss: 4.2098822593688965\n",
      "Valdiation Avg Score:  6.241357157348974\n",
      "Epoch 43\n",
      "Loss: 4.38345193862915\n",
      "New model saved! better average RMCMSE score  1.365982570852714\n",
      "Valdiation Avg Score:  6.600247422630064\n",
      "Epoch 44\n",
      "Loss: 4.23347282409668\n",
      "New model saved! better average RMCMSE score  1.3616449856924853\n",
      "Valdiation Avg Score:  6.3127172668944205\n",
      "Epoch 45\n",
      "Loss: 4.434970855712891\n",
      "New model saved! better average RMCMSE score  1.357379979073604\n",
      "Valdiation Avg Score:  6.484283218642749\n",
      "Epoch 46\n",
      "Loss: 4.186771869659424\n",
      "Valdiation Avg Score:  7.067929302461467\n",
      "Epoch 47\n",
      "Loss: 4.211463928222656\n",
      "Valdiation Avg Score:  6.852909857392017\n",
      "Epoch 48\n",
      "Loss: 3.91463565826416\n",
      "Valdiation Avg Score:  7.475755142594033\n",
      "Epoch 49\n",
      "Loss: 3.7670154571533203\n",
      "New model saved! better average RMCMSE score  1.3519384058140638\n",
      "Valdiation Avg Score:  6.223838733788945\n",
      "Epoch 50\n",
      "Loss: 3.9791502952575684\n",
      "New model saved! better average RMCMSE score  1.3505701245577602\n",
      "Average loss:  3.954751212718123\n",
      "MCRmse average score:  1.3505701245577602\n",
      "Training accuracy!\n",
      "Cohesion:  0.0936742934051144  Syntax:  0.1297442799461642  Vocab:  0.06810228802153433\n",
      "Phraseology:  0.10013458950201884  Grammar:  0.1388963660834455  Conventions:  0.12355316285329744\n",
      "Valdiation Avg Score:  6.220686340673924\n",
      "Epoch 51\n",
      "Loss: 3.9667694568634033\n",
      "New model saved! better average RMCMSE score  1.3420094696193046\n",
      "Valdiation Avg Score:  6.304908582097777\n",
      "Epoch 52\n",
      "Loss: 3.4094467163085938\n",
      "Valdiation Avg Score:  6.974273963773289\n",
      "Epoch 53\n",
      "Loss: 3.4409339427948\n",
      "Valdiation Avg Score:  6.46792639036405\n",
      "Epoch 54\n",
      "Loss: 3.5502243041992188\n",
      "New model saved! better average RMCMSE score  1.3377928736712983\n",
      "Valdiation Avg Score:  7.334548182654775\n",
      "Epoch 55\n",
      "Loss: 3.608867645263672\n",
      "Valdiation Avg Score:  7.050990186328888\n",
      "Epoch 56\n",
      "Loss: 3.0032835006713867\n",
      "New model saved! better average RMCMSE score  1.331928325406641\n",
      "Valdiation Avg Score:  7.296457498354058\n",
      "Epoch 57\n",
      "Loss: 2.676501750946045\n",
      "Valdiation Avg Score:  6.082521791086048\n",
      "Epoch 58\n",
      "Loss: 2.5189037322998047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valdiation Avg Score:  6.440729884636751\n",
      "Epoch 59\n",
      "Loss: 2.9732625484466553\n",
      "Valdiation Avg Score:  6.303834462203202\n",
      "Epoch 60\n",
      "Loss: 3.4936985969543457\n",
      "New model saved! better average RMCMSE score  1.3295107728501832\n",
      "Average loss:  3.1511811967623435\n",
      "MCRmse average score:  1.3295107728501832\n",
      "Training accuracy!\n",
      "Cohesion:  0.08209959623149395  Syntax:  0.12139973082099596  Vocab:  0.04495289367429341\n",
      "Phraseology:  0.09986541049798116  Grammar:  0.13566621803499326  Conventions:  0.12032301480484522\n",
      "Valdiation Avg Score:  7.644958763080764\n",
      "Epoch 61\n",
      "Loss: 3.0555410385131836\n",
      "New model saved! better average RMCMSE score  1.3229840194514848\n",
      "Valdiation Avg Score:  5.7140864077724185\n",
      "Epoch 62\n",
      "Loss: 2.8617007732391357\n",
      "Valdiation Avg Score:  5.974027154748553\n",
      "Epoch 63\n",
      "Loss: 3.3230271339416504\n",
      "Valdiation Avg Score:  6.1893547968974705\n",
      "Epoch 64\n",
      "Loss: 2.904573678970337\n",
      "Valdiation Avg Score:  7.4673883619246935\n",
      "Epoch 65\n",
      "Loss: 2.906820058822632\n",
      "Valdiation Avg Score:  7.272625740546034\n",
      "Epoch 66\n",
      "Loss: 2.114187717437744\n",
      "Valdiation Avg Score:  5.939368636888798\n",
      "Epoch 67\n",
      "Loss: 2.7410154342651367\n",
      "Valdiation Avg Score:  6.552097777445972\n",
      "Epoch 68\n",
      "Loss: 2.552001953125\n",
      "New model saved! better average RMCMSE score  1.3187068839340648\n",
      "Valdiation Avg Score:  6.479570124902251\n",
      "Epoch 69\n",
      "Loss: 2.759413242340088\n",
      "Valdiation Avg Score:  6.371834141429402\n",
      "Epoch 70\n",
      "Loss: 2.863051652908325\n",
      "Average loss:  2.612789442983724\n",
      "MCRmse average score:  1.3223433718040472\n",
      "Training accuracy!\n",
      "Cohesion:  0.08963660834454913  Syntax:  0.11816958277254375  Vocab:  0.040376850605652756\n",
      "Phraseology:  0.09125168236877523  Grammar:  0.14347240915208614  Conventions:  0.11816958277254375\n",
      "Valdiation Avg Score:  6.64837931231378\n",
      "Epoch 71\n",
      "Loss: 2.449587106704712\n",
      "Valdiation Avg Score:  6.981658497374086\n",
      "Epoch 72\n",
      "Loss: 2.2409090995788574\n",
      "Valdiation Avg Score:  6.494209653379934\n",
      "Epoch 73\n",
      "Loss: 2.1836841106414795\n",
      "Valdiation Avg Score:  6.4857176380282695\n",
      "Epoch 74\n",
      "Loss: 2.3965909481048584\n",
      "Valdiation Avg Score:  6.2364129686986285\n",
      "Epoch 75\n",
      "Loss: 2.361133098602295\n",
      "New model saved! better average RMCMSE score  1.318578345449028\n",
      "Valdiation Avg Score:  7.317994372612259\n",
      "Epoch 76\n",
      "Loss: 2.237400531768799\n",
      "New model saved! better average RMCMSE score  1.317724776981493\n",
      "Valdiation Avg Score:  6.390723060505589\n",
      "Epoch 77\n",
      "Loss: 2.0334601402282715\n",
      "New model saved! better average RMCMSE score  1.316489847689059\n",
      "Valdiation Avg Score:  6.895085172770299\n",
      "Epoch 78\n",
      "Loss: 2.321681022644043\n",
      "Valdiation Avg Score:  6.737504055144669\n",
      "Epoch 79\n",
      "Loss: 1.738424301147461\n",
      "Valdiation Avg Score:  6.3624357801483375\n",
      "Epoch 80\n",
      "Loss: 2.191411018371582\n",
      "New model saved! better average RMCMSE score  1.3164898428697223\n",
      "Average loss:  2.2137958053815163\n",
      "MCRmse average score:  1.3164898428697223\n",
      "Training accuracy!\n",
      "Cohesion:  0.08371467025572005  Syntax:  0.11655450874831763  Vocab:  0.045491251682368776\n",
      "Phraseology:  0.09528936742934051  Grammar:  0.1458950201884253  Conventions:  0.11790040376850605\n",
      "Valdiation Avg Score:  7.016033619042137\n",
      "Epoch 81\n",
      "Loss: 1.9926692247390747\n",
      "Valdiation Avg Score:  6.719663858856051\n",
      "Epoch 82\n",
      "Loss: 2.305624008178711\n",
      "Valdiation Avg Score:  7.246897410589425\n",
      "Epoch 83\n",
      "Loss: 2.1136486530303955\n",
      "New model saved! better average RMCMSE score  1.314158524215825\n",
      "Valdiation Avg Score:  6.195755754946535\n",
      "Epoch 84\n",
      "Loss: 1.7656649351119995\n",
      "New model saved! better average RMCMSE score  1.3128050156784559\n",
      "Valdiation Avg Score:  7.075098167328186\n",
      "Epoch 85\n",
      "Loss: 2.2397022247314453\n",
      "Valdiation Avg Score:  6.968284028276437\n",
      "Epoch 86\n",
      "Loss: 2.102245330810547\n",
      "New model saved! better average RMCMSE score  1.3108195175736026\n",
      "Valdiation Avg Score:  6.516695775529335\n",
      "Epoch 87\n",
      "Loss: 1.5074385404586792\n",
      "Valdiation Avg Score:  7.768234956623244\n",
      "Epoch 88\n",
      "Loss: 1.6482634544372559\n",
      "Valdiation Avg Score:  6.017839632645351\n",
      "Epoch 89\n",
      "Loss: 1.4992334842681885\n",
      "New model saved! better average RMCMSE score  1.3042352256256005\n",
      "Valdiation Avg Score:  7.620963334054962\n",
      "Epoch 90\n",
      "Loss: 1.7368979454040527\n",
      "Average loss:  1.8847288923748469\n",
      "MCRmse average score:  1.3124366704606343\n",
      "Training accuracy!\n",
      "Cohesion:  0.08775235531628534  Syntax:  0.11547779273216689  Vocab:  0.04306864064602961\n",
      "Phraseology:  0.09232839838492597  Grammar:  0.13808882907133244  Conventions:  0.11736204576043069\n",
      "Valdiation Avg Score:  7.500257587684219\n",
      "Epoch 91\n",
      "Loss: 1.988461971282959\n",
      "New model saved! better average RMCMSE score  1.3032202054243032\n",
      "Valdiation Avg Score:  6.288496320467444\n",
      "Epoch 92\n",
      "Loss: 1.5312037467956543\n",
      "Valdiation Avg Score:  6.5095478361159795\n",
      "Epoch 93\n",
      "Loss: 2.0476808547973633\n",
      "Valdiation Avg Score:  6.517471591873836\n",
      "Epoch 94\n",
      "Loss: 1.9410110712051392\n",
      "Valdiation Avg Score:  6.544028064329554\n",
      "Epoch 95\n",
      "Loss: 1.4376245737075806\n",
      "Valdiation Avg Score:  7.24945452328688\n",
      "Epoch 96\n",
      "Loss: 1.5356872081756592\n",
      "Valdiation Avg Score:  7.511191074047963\n",
      "Epoch 97\n",
      "Loss: 1.8973829746246338\n",
      "Valdiation Avg Score:  6.503391862511712\n",
      "Epoch 98\n",
      "Loss: 1.7449665069580078\n",
      "Valdiation Avg Score:  6.6320845036435765\n",
      "Epoch 99\n",
      "Loss: 1.600097894668579\n",
      "Valdiation Avg Score:  6.365632053631288\n",
      "Epoch 100\n",
      "Loss: 1.5352610349655151\n",
      "Average loss:  1.6333477416280973\n",
      "MCRmse average score:  1.3060458902533887\n",
      "Training accuracy!\n",
      "Cohesion:  0.08317631224764468  Syntax:  0.1135935397039031  Vocab:  0.03983849259757739\n",
      "Phraseology:  0.09636608344549125  Grammar:  0.13997308209959622  Conventions:  0.11117092866756394\n",
      "Valdiation Avg Score:  6.664880684266446\n",
      "Epoch 101\n",
      "Loss: 1.332548975944519\n",
      "Valdiation Avg Score:  6.278702957740565\n",
      "Epoch 102\n",
      "Loss: 1.5346543788909912\n",
      "Valdiation Avg Score:  6.572830230683148\n",
      "Epoch 103\n",
      "Loss: 1.2974724769592285\n",
      "Valdiation Avg Score:  6.698273251402338\n",
      "Epoch 104\n",
      "Loss: 1.1490416526794434\n",
      "New model saved! better average RMCMSE score  1.299884892443229\n",
      "Valdiation Avg Score:  6.586759130516615\n",
      "Epoch 105\n",
      "Loss: 1.7426660060882568\n",
      "Valdiation Avg Score:  7.0049471779009895\n",
      "Epoch 106\n",
      "Loss: 2.0886433124542236\n",
      "Valdiation Avg Score:  6.335209425619729\n",
      "Epoch 107\n",
      "Loss: 1.2121076583862305\n",
      "Valdiation Avg Score:  7.1038178467150335\n",
      "Epoch 108\n",
      "Loss: 1.6163221597671509\n",
      "New model saved! better average RMCMSE score  1.2979788929933873\n",
      "Valdiation Avg Score:  7.1499030683622244\n",
      "Epoch 109\n",
      "Loss: 1.573451280593872\n",
      "Valdiation Avg Score:  6.356963900648634\n",
      "Epoch 110\n",
      "Loss: 1.3835017681121826\n",
      "Average loss:  1.5435469655667322\n",
      "MCRmse average score:  1.3088786805068082\n",
      "Training accuracy!\n",
      "Cohesion:  0.08775235531628534  Syntax:  0.11386271870794078  Vocab:  0.04226110363391655\n",
      "Phraseology:  0.09475100942126514  Grammar:  0.1391655450874832  Conventions:  0.11063257065948856\n",
      "Valdiation Avg Score:  6.635208233509784\n",
      "Epoch 111\n",
      "Loss: 1.1584324836730957\n",
      "Valdiation Avg Score:  7.0213770958525465\n",
      "Epoch 112\n",
      "Loss: 1.68973970413208\n",
      "Valdiation Avg Score:  6.401841645701378\n",
      "Epoch 113\n",
      "Loss: 1.3283398151397705\n",
      "Valdiation Avg Score:  6.242633883085084\n",
      "Epoch 114\n",
      "Loss: 1.2143819332122803\n",
      "Valdiation Avg Score:  6.3068250618051875\n",
      "Epoch 115\n",
      "Loss: 1.4874317646026611\n",
      "Valdiation Avg Score:  6.835025561318488\n",
      "Epoch 116\n",
      "Loss: 0.9694436192512512\n",
      "Valdiation Avg Score:  6.197974301543809\n",
      "Epoch 117\n",
      "Loss: 1.6286795139312744\n",
      "Valdiation Avg Score:  6.961342515389717\n",
      "Epoch 118\n",
      "Loss: 1.418281078338623\n",
      "New model saved! better average RMCMSE score  1.2979368743809003\n",
      "Valdiation Avg Score:  7.07352333256677\n",
      "Epoch 119\n",
      "Loss: 1.1942099332809448\n",
      "Valdiation Avg Score:  6.012184702772162\n",
      "Epoch 120\n",
      "Loss: 1.352813482284546\n",
      "Average loss:  1.412104467214164\n",
      "MCRmse average score:  1.3009916165141182\n",
      "Training accuracy!\n",
      "Cohesion:  0.08371467025572005  Syntax:  0.11036339165545088  Vocab:  0.03714670255720054\n",
      "Phraseology:  0.09663526244952894  Grammar:  0.14697173620457604  Conventions:  0.10497981157469717\n",
      "Valdiation Avg Score:  7.733242825517619\n",
      "Epoch 121\n",
      "Loss: 1.5349401235580444\n",
      "New model saved! better average RMCMSE score  1.2961929698315762\n",
      "Valdiation Avg Score:  7.063816084022388\n",
      "Epoch 122\n",
      "Loss: 0.8004757165908813\n",
      "Valdiation Avg Score:  7.032457343415744\n",
      "Epoch 123\n",
      "Loss: 1.021997332572937\n",
      "Valdiation Avg Score:  7.099356393070221\n",
      "Epoch 124\n",
      "Loss: 1.1776907444000244\n",
      "Valdiation Avg Score:  6.835222971716915\n",
      "Epoch 125\n",
      "Loss: 1.263796091079712\n",
      "Valdiation Avg Score:  6.646706419142278\n",
      "Epoch 126\n",
      "Loss: 1.2074298858642578\n",
      "Valdiation Avg Score:  6.421477751786218\n",
      "Epoch 127\n",
      "Loss: 1.143728256225586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valdiation Avg Score:  6.347886242466917\n",
      "Epoch 128\n",
      "Loss: 0.9425005316734314\n",
      "Valdiation Avg Score:  6.982790575340987\n",
      "Epoch 129\n",
      "Loss: 1.028239130973816\n",
      "Valdiation Avg Score:  7.303817212354106\n",
      "Epoch 130\n",
      "Loss: 0.8157036304473877\n",
      "Average loss:  1.3226053159115678\n",
      "MCRmse average score:  1.30447616291813\n",
      "Training accuracy!\n",
      "Cohesion:  0.08398384925975774  Syntax:  0.10874831763122476  Vocab:  0.03526244952893674\n",
      "Phraseology:  0.09259757738896367  Grammar:  0.1440107671601615  Conventions:  0.11197846567967698\n",
      "Valdiation Avg Score:  6.469885425960305\n",
      "Epoch 131\n",
      "Loss: 0.9140227437019348\n",
      "Valdiation Avg Score:  6.892645710341861\n",
      "Epoch 132\n",
      "Loss: 0.879353404045105\n",
      "Valdiation Avg Score:  6.727481706852082\n",
      "Epoch 133\n",
      "Loss: 0.9392303228378296\n",
      "New model saved! better average RMCMSE score  1.2933026544958375\n",
      "Valdiation Avg Score:  6.658221747198617\n",
      "Epoch 134\n",
      "Loss: 1.4672948122024536\n",
      "Valdiation Avg Score:  6.925335144958586\n",
      "Epoch 135\n",
      "Loss: 1.300905466079712\n",
      "Valdiation Avg Score:  6.680457871390596\n",
      "Epoch 136\n",
      "Loss: 0.8715025782585144\n",
      "Valdiation Avg Score:  6.029756251246462\n",
      "Epoch 137\n",
      "Loss: 0.7623099684715271\n",
      "Valdiation Avg Score:  6.671575928024348\n",
      "Epoch 138\n",
      "Loss: 1.0076807737350464\n",
      "Valdiation Avg Score:  5.839269758004175\n",
      "Epoch 139\n",
      "Loss: 1.0397045612335205\n",
      "Valdiation Avg Score:  6.53976520287722\n",
      "Epoch 140\n",
      "Loss: 1.304740309715271\n",
      "Average loss:  1.298104497335725\n",
      "MCRmse average score:  1.3006260646052092\n",
      "Training accuracy!\n",
      "Cohesion:  0.08532974427994616  Syntax:  0.10928667563930014  Vocab:  0.03580080753701211\n",
      "Phraseology:  0.09232839838492597  Grammar:  0.13970390309555855  Conventions:  0.11144010767160162\n",
      "Valdiation Avg Score:  6.424690214995418\n",
      "Epoch 141\n",
      "Loss: 1.3770451545715332\n",
      "New model saved! better average RMCMSE score  1.2906952919584047\n",
      "Valdiation Avg Score:  6.952436987210765\n",
      "Epoch 142\n",
      "Loss: 0.9006727337837219\n",
      "Valdiation Avg Score:  6.368583813846001\n",
      "Epoch 143\n",
      "Loss: 0.7441495656967163\n",
      "Valdiation Avg Score:  6.617233632255012\n",
      "Epoch 144\n",
      "Loss: 1.0242767333984375\n",
      "Valdiation Avg Score:  6.518609734093674\n",
      "Epoch 145\n",
      "Loss: 0.9160559177398682\n",
      "Valdiation Avg Score:  7.332780237800005\n",
      "Epoch 146\n",
      "Loss: 1.3400945663452148\n",
      "New model saved! better average RMCMSE score  1.2902154418451803\n",
      "Valdiation Avg Score:  6.824623248975353\n",
      "Epoch 147\n",
      "Loss: 1.0823479890823364\n",
      "Valdiation Avg Score:  6.690519601125014\n",
      "Epoch 148\n",
      "Loss: 1.0588905811309814\n",
      "Valdiation Avg Score:  6.698048849839231\n",
      "Epoch 149\n",
      "Loss: 0.877129852771759\n",
      "Valdiation Avg Score:  6.2730992620120745\n",
      "Epoch 150\n",
      "Loss: 1.0094455480575562\n",
      "Average loss:  1.1608828387017978\n",
      "MCRmse average score:  1.3017789267770827\n",
      "Training accuracy!\n",
      "Cohesion:  0.08317631224764468  Syntax:  0.10874831763122476  Vocab:  0.03687752355316285\n",
      "Phraseology:  0.09286675639300135  Grammar:  0.14185733512786003  Conventions:  0.11036339165545088\n",
      "Valdiation Avg Score:  6.443969711131038\n",
      "Epoch 151\n",
      "Loss: 0.9513489007949829\n",
      "Valdiation Avg Score:  6.382242393749218\n",
      "Epoch 152\n",
      "Loss: 1.1761189699172974\n",
      "Valdiation Avg Score:  6.0765004536772445\n",
      "Epoch 153\n",
      "Loss: 0.9621425867080688\n",
      "Valdiation Avg Score:  6.7471136388304\n",
      "Epoch 154\n",
      "Loss: 0.7099080085754395\n",
      "Valdiation Avg Score:  6.598401955520994\n",
      "Epoch 155\n",
      "Loss: 0.9092189073562622\n",
      "Valdiation Avg Score:  6.107745476859453\n",
      "Epoch 156\n",
      "Loss: 0.9630103707313538\n",
      "Valdiation Avg Score:  7.2382909769083685\n",
      "Epoch 157\n",
      "Loss: 0.9749422669410706\n",
      "Valdiation Avg Score:  6.32649910614906\n",
      "Epoch 158\n",
      "Loss: 1.0631682872772217\n",
      "Valdiation Avg Score:  6.783553527644656\n",
      "Epoch 159\n",
      "Loss: 0.5905612707138062\n",
      "Valdiation Avg Score:  7.13141781178257\n",
      "Epoch 160\n",
      "Loss: 1.0922073125839233\n",
      "Average loss:  1.1667759226540388\n",
      "MCRmse average score:  1.3018857498480028\n",
      "Training accuracy!\n",
      "Cohesion:  0.08263795423956931  Syntax:  0.1117092866756393  Vocab:  0.03714670255720054\n",
      "Phraseology:  0.09313593539703903  Grammar:  0.14320323014804845  Conventions:  0.10659488559892329\n",
      "Valdiation Avg Score:  5.9024136704667285\n",
      "Epoch 161\n",
      "Loss: 1.1478033065795898\n",
      "Valdiation Avg Score:  6.491044846801047\n",
      "Epoch 162\n",
      "Loss: 1.1475774049758911\n",
      "Valdiation Avg Score:  6.514567223659601\n",
      "Epoch 163\n",
      "Loss: 1.1377522945404053\n",
      "Valdiation Avg Score:  6.437190451877277\n",
      "Epoch 164\n",
      "Loss: 1.2332621812820435\n",
      "Valdiation Avg Score:  7.125925146364692\n",
      "Epoch 165\n",
      "Loss: 1.0093727111816406\n",
      "Valdiation Avg Score:  6.362852083035428\n",
      "Epoch 166\n",
      "Loss: 0.8650221824645996\n",
      "New model saved! better average RMCMSE score  1.285106803912832\n",
      "Valdiation Avg Score:  6.435780447658473\n",
      "Epoch 167\n",
      "Loss: 0.6726682186126709\n",
      "Valdiation Avg Score:  6.138571367916766\n",
      "Epoch 168\n",
      "Loss: 0.803231418132782\n",
      "Valdiation Avg Score:  6.74232171814287\n",
      "Epoch 169\n",
      "Loss: 0.7549965977668762\n",
      "Valdiation Avg Score:  6.254227265834568\n",
      "Epoch 170\n",
      "Loss: 0.8479381799697876\n",
      "Average loss:  1.0092187049025196\n",
      "MCRmse average score:  1.3009961652218403\n",
      "Training accuracy!\n",
      "Cohesion:  0.0845222072678331  Syntax:  0.1079407806191117  Vocab:  0.033647375504710635\n",
      "Phraseology:  0.09125168236877523  Grammar:  0.1423956931359354  Conventions:  0.10417227456258411\n",
      "Valdiation Avg Score:  7.024368168391023\n",
      "Epoch 171\n",
      "Loss: 0.9763194918632507\n",
      "Valdiation Avg Score:  6.61178900633129\n",
      "Epoch 172\n",
      "Loss: 0.9918650984764099\n",
      "Valdiation Avg Score:  6.656561270741277\n",
      "Epoch 173\n",
      "Loss: 0.9253361821174622\n",
      "Valdiation Avg Score:  6.831207223065425\n",
      "Epoch 174\n",
      "Loss: 1.092482089996338\n",
      "Valdiation Avg Score:  7.382242125806876\n",
      "Epoch 175\n",
      "Loss: 1.1731126308441162\n",
      "Valdiation Avg Score:  6.456098676681872\n",
      "Epoch 176\n",
      "Loss: 0.8279251456260681\n",
      "Valdiation Avg Score:  6.109103473810063\n",
      "Epoch 177\n",
      "Loss: 0.7786931991577148\n",
      "Valdiation Avg Score:  6.458855148410128\n",
      "Epoch 178\n",
      "Loss: 0.8466506004333496\n",
      "Valdiation Avg Score:  6.834915500940831\n",
      "Epoch 179\n",
      "Loss: 1.0806926488876343\n",
      "Valdiation Avg Score:  6.143236189756674\n",
      "Epoch 180\n",
      "Loss: 0.9738316535949707\n",
      "Average loss:  1.0289896150766793\n",
      "MCRmse average score:  1.294199048215199\n",
      "Training accuracy!\n",
      "Cohesion:  0.08236877523553163  Syntax:  0.11009421265141318  Vocab:  0.03553162853297443\n",
      "Phraseology:  0.09232839838492597  Grammar:  0.1426648721399731  Conventions:  0.10686406460296097\n",
      "Valdiation Avg Score:  6.4497529780940726\n",
      "Epoch 181\n",
      "Loss: 0.9560912251472473\n",
      "Valdiation Avg Score:  6.6942268927681585\n",
      "Epoch 182\n",
      "Loss: 1.400444507598877\n",
      "Valdiation Avg Score:  6.084304669954526\n",
      "Epoch 183\n",
      "Loss: 1.1331793069839478\n",
      "Valdiation Avg Score:  6.487571279824708\n",
      "Epoch 184\n",
      "Loss: 0.7907981872558594\n",
      "Valdiation Avg Score:  6.646284403234837\n",
      "Epoch 185\n",
      "Loss: 1.0598429441452026\n",
      "Valdiation Avg Score:  6.803844552261394\n",
      "Epoch 186\n",
      "Loss: 0.7724226117134094\n",
      "Valdiation Avg Score:  6.419894368868183\n",
      "Epoch 187\n",
      "Loss: 1.050499439239502\n",
      "Valdiation Avg Score:  6.771049915524241\n",
      "Epoch 188\n",
      "Loss: 0.9483509063720703\n",
      "Valdiation Avg Score:  7.536270375739422\n",
      "Epoch 189\n",
      "Loss: 1.261208415031433\n",
      "Valdiation Avg Score:  6.533146717125718\n",
      "Epoch 190\n",
      "Loss: 1.0052295923233032\n",
      "Average loss:  1.0627566414364313\n",
      "MCRmse average score:  1.2960984177140171\n",
      "Training accuracy!\n",
      "Cohesion:  0.0847913862718708  Syntax:  0.10955585464333782  Vocab:  0.03580080753701211\n",
      "Phraseology:  0.09205921938088829  Grammar:  0.1388963660834455  Conventions:  0.11144010767160162\n",
      "Valdiation Avg Score:  6.194419733711468\n",
      "Epoch 191\n",
      "Loss: 0.9533822536468506\n",
      "Valdiation Avg Score:  7.49783268269749\n",
      "Epoch 192\n",
      "Loss: 0.9400688409805298\n",
      "Valdiation Avg Score:  6.542956399188556\n",
      "Epoch 193\n",
      "Loss: 1.2281547784805298\n",
      "Valdiation Avg Score:  6.145865452574836\n",
      "Epoch 194\n",
      "Loss: 0.8091484308242798\n",
      "Valdiation Avg Score:  6.64936569600637\n",
      "Epoch 195\n",
      "Loss: 0.9588913917541504\n",
      "Valdiation Avg Score:  7.396202846438164\n",
      "Epoch 196\n",
      "Loss: 0.6604592800140381\n",
      "Valdiation Avg Score:  6.133864648058799\n",
      "Epoch 197\n",
      "Loss: 0.8806043267250061\n",
      "Valdiation Avg Score:  6.801562613506279\n",
      "Epoch 198\n",
      "Loss: 0.7899850606918335\n",
      "Valdiation Avg Score:  6.606889024741293\n",
      "Epoch 199\n",
      "Loss: 0.9075630307197571\n",
      "Valdiation Avg Score:  6.367672259366518\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "max_epochs = 200\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "best_loss = 9999\n",
    "test_score = 0.35\n",
    "save_path = 'test/best_model_v5.pth'\n",
    "for epoch in range(max_epochs):\n",
    "    all_losses = [] \n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    coh_train_correct = 0\n",
    "    syn_train_correct = 0\n",
    "    voc_train_correct = 0\n",
    "    phr_train_correct = 0\n",
    "    gr_train_correct = 0\n",
    "    con_train_correct = 0\n",
    "    train_samples = 0\n",
    "    mcrmse_s = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        # self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "        text,cohesion,syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = batch_data\n",
    "        outputs = model(text,text_length,pos_tag)\n",
    "\n",
    "\n",
    "        # loss\n",
    "        loss = 0\n",
    "        loss += loss_func(outputs['cohesion'], cohesion)\n",
    "        loss += loss_func(outputs['syntax'], syntax)\n",
    "        loss += loss_func(outputs['vocabulary'], vocab)\n",
    "        loss += loss_func(outputs['phraseology'], phraseology)\n",
    "        loss += loss_func(outputs['grammar'], grammer)\n",
    "        loss += loss_func(outputs['conventions'], conventions)\n",
    "        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Loss: {}\".format(loss.item()))\n",
    "\n",
    "        coh_acc,coh_true = compare_tensor_acc(outputs['cohesion'],cohesion)\n",
    "        syn_acc,syn_true = compare_tensor_acc(outputs['syntax'],syntax)\n",
    "        voc_acc,voc_true = compare_tensor_acc(outputs['vocabulary'],vocab)\n",
    "        phr_acc,phr_true = compare_tensor_acc(outputs['phraseology'],phraseology)\n",
    "        gr_acc,gr_true = compare_tensor_acc(outputs['grammar'],grammer)\n",
    "        con_acc,con_true = compare_tensor_acc(outputs['conventions'],conventions)\n",
    "\n",
    "\n",
    "        scores = []\n",
    "        scores.append([coh_acc,cohesion])\n",
    "        scores.append([syn_acc,syntax])\n",
    "        scores.append([voc_acc,vocab])\n",
    "        scores.append([phr_acc,phraseology])\n",
    "        scores.append([gr_acc,grammer])\n",
    "        scores.append([con_acc,conventions])\n",
    "\n",
    "        batch_score = mcrmse_score(scores)\n",
    "        mcrmse_s += batch_score\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print(\"MCRmse batch score: \", batch_score)\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        coh_train_correct += coh_true\n",
    "        syn_train_correct += syn_true\n",
    "        voc_train_correct += voc_true\n",
    "        phr_train_correct += phr_true\n",
    "        gr_train_correct += gr_true\n",
    "        con_train_correct += con_true\n",
    "        train_samples+= len(outputs['cohesion'].tolist())\n",
    "    def avg(lst):\n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "    # calculating mcrmse metric\n",
    "    mcrmse_epoch_score = mcrmse_s/len(train_loader)\n",
    "    if mcrmse_epoch_score < best_loss:\n",
    "        best_loss = mcrmse_epoch_score\n",
    "        save_model(model,save_path)\n",
    "        print(\"New model saved! better average RMCMSE score \", best_loss)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Average loss: \", avg(all_losses))\n",
    "        print(\"MCRmse average score: \", mcrmse_s/len(train_loader))\n",
    "        print(\"Training accuracy!\")\n",
    "        print(\"Cohesion: \", coh_train_correct/train_samples, \" Syntax: \", syn_train_correct/train_samples, \" Vocab: \", voc_train_correct/train_samples)\n",
    "        print(\"Phraseology: \", phr_train_correct/train_samples, \" Grammar: \", gr_train_correct/train_samples, \" Conventions: \", con_train_correct/train_samples)\n",
    "    \n",
    "    ## validation\n",
    "    coh_correct = 0\n",
    "    syntax_correct = 0\n",
    "    vocab_correct = 0\n",
    "    phraseology_correct = 0\n",
    "    grammar_correct = 0\n",
    "    conventions_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    coh_ans = []\n",
    "    syntax_ans = []\n",
    "    voc_ans = []\n",
    "    phr_ans = []\n",
    "    gra_ans = []\n",
    "    conv_ans = []\n",
    "    text_original = []\n",
    "    valid_score = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(valid_loader):\n",
    "            text,cohesion,syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = batch_data\n",
    "            outputs = model(text,text_length,pos_tag)\n",
    "            coh_acc,coh_true = compare_tensor_acc(outputs['cohesion'],cohesion)\n",
    "            syn_acc,syn_true = compare_tensor_acc(outputs['syntax'],syntax)\n",
    "            voc_acc,voc_true = compare_tensor_acc(outputs['vocabulary'],vocab)\n",
    "            phr_acc,phr_true = compare_tensor_acc(outputs['phraseology'],phraseology)\n",
    "            gr_acc,gr_true = compare_tensor_acc(outputs['grammar'],grammer)\n",
    "            con_acc,con_true = compare_tensor_acc(outputs['conventions'],conventions)\n",
    "\n",
    "\n",
    "            scores = []\n",
    "            scores.append([coh_acc,cohesion])\n",
    "            scores.append([syn_acc,syntax])\n",
    "            scores.append([voc_acc,vocab])\n",
    "            scores.append([phr_acc,phraseology])\n",
    "            scores.append([gr_acc,grammer])\n",
    "            scores.append([con_acc,conventions])\n",
    "\n",
    "            batch_score = mcrmse_score(scores)\n",
    "            valid_score += batch_score\n",
    "\n",
    "\n",
    "    print(\"Valdiation Avg Score: \" , np.mean(valid_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "class GrammerTestDataset(Dataset):\n",
    "    pos_tag_vocab = ['CC',\n",
    "            'WRB',\n",
    "            'EX',\n",
    "            'MD',\n",
    "            'VBN',\n",
    "            'VBD',\n",
    "            'NNS',\n",
    "            'RBR',\n",
    "            'VBZ',\n",
    "            'PRP$',\n",
    "            'VB',\n",
    "            'RP',\n",
    "            'WP',\n",
    "            'VBP',\n",
    "            'JJR',\n",
    "            'VBG',\n",
    "            'PDT',\n",
    "            'JJ',\n",
    "            'JJS',\n",
    "            'WDT',\n",
    "            'IN',\n",
    "            'DT',\n",
    "            'RB',\n",
    "            'NN',\n",
    "            'PRP',\n",
    "            'TO']\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Dataset object for base model\n",
    "        :param data:\n",
    "        '''\n",
    "        self.text,self.text_length,self.pos_tag = self.clean_text(data['full_text'])\n",
    "        self.text_labels = data['text_id'].to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def feature_transformation(self, vals):\n",
    "        self.mapping = {1:0,1.5:1,2:2,2.5:3,3:4,3.5:5,4:6,4.5:7,5:8}\n",
    "        newl = []\n",
    "        for v in vals:\n",
    "            newl.append(self.mapping[v])\n",
    "        return newl\n",
    "    \n",
    "    def count_pos_tag(self,text):\n",
    "        tag_dict = {}\n",
    "        pos = nltk.pos_tag(text)\n",
    "        tag_types = list(set([item[1] for item in pos]))\n",
    "        for tag in tag_types:\n",
    "            tag_dict[tag] = sum([item[1]==tag for item in pos])\n",
    "        return tag_dict\n",
    "\n",
    "    def clean_text(self,all_text):\n",
    "        newl = []\n",
    "        newlength = []\n",
    "        pos_tag = []\n",
    "        for text in all_text:\n",
    "            text = text.replace(\"\\n\",\" \").lower()\n",
    "            text = text.strip()\n",
    "            newl.append(text)\n",
    "            newlength.append([len(t) for t in text.split()])\n",
    "            pos_sentence = self.count_pos_tag(text.split())\n",
    "            hot_vector_pos = []\n",
    "            for po in self.pos_tag_vocab:\n",
    "                if po in pos_sentence.keys():\n",
    "                    hot_vector_pos.append(pos_sentence[po])\n",
    "                else:\n",
    "                    hot_vector_pos.append(0)\n",
    "            pos_tag.append(hot_vector_pos)\n",
    "        return np.array(newl),np.array(newlength),np.array(pos_tag)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx],self.text_length[idx],self.pos_tag[idx],self.text_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>when a person has no experience on a job their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>Do you think students would benefit from being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text\n",
       "0  0000C359D63E  when a person has no experience on a job their...\n",
       "1  000BAD50D026  Do you think students would benefit from being...\n",
       "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(path+\"test.csv\")\n",
    "testdata = GrammerTestDataset(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "def collate_test_batch(batch):\n",
    "    text_list,text_length,pos_tag,text_id = [], [],[],[]\n",
    "    for data in batch:\n",
    "        processed_text = text_pipeline(data[0],1024)\n",
    "        text_list.append(processed_text)\n",
    "        text_length.append(len(processed_text))\n",
    "        pos_tag.append(data[2])\n",
    "        text_id.append(data[3])\n",
    "        \n",
    "        \n",
    "        \n",
    "    text_length = torch.tensor(text_length, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    pos_tag = torch.tensor(pos_tag, dtype=torch.int64)\n",
    "    return text_list.to(device), text_length.to(device),pos_tag.to(device),text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model,loader,return_preds=False):\n",
    "    coh_correct = 0\n",
    "    syntax_correct = 0\n",
    "    vocab_correct = 0\n",
    "    phraseology_correct = 0\n",
    "    grammar_correct = 0\n",
    "    conventions_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    coh_ans = []\n",
    "    syntax_ans = []\n",
    "    voc_ans = []\n",
    "    phr_ans = []\n",
    "    gra_ans = []\n",
    "    conv_ans = []\n",
    "    text_original = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(loader):\n",
    "            # self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "            text = batch_data[0]\n",
    "            text_length = batch_data[1]\n",
    "            pos_tag = batch_data[2]\n",
    "            text_id = batch_data[3]\n",
    "            outputs = model(text,text_length,pos_tag)\n",
    "\n",
    "            labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "            \n",
    "            \n",
    "            # cohesion \n",
    "            #a, predicted = torch.max(outputs['cohesion'],1)\n",
    "            preds = torch.softmax(outputs['cohesion'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [coh_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "            \n",
    "             # syntax \n",
    "            preds = torch.softmax(outputs['syntax'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [syntax_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "            \n",
    "            # vocab \n",
    "            preds = torch.softmax(outputs['vocabulary'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [voc_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "\n",
    "             # phraseology    \n",
    "            preds = torch.softmax(outputs['phraseology'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [phr_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "            # grammar \n",
    "            preds = torch.softmax(outputs['grammar'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [gra_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "            \n",
    "\n",
    "            # conventions \n",
    "            preds = torch.softmax(outputs['conventions'], dim=1)\n",
    "            proba, class_label = preds.max(dim=1)\n",
    "            [conv_ans.append((transform_labels(class_label), proba.item())) for res in preds]\n",
    "\n",
    "            total_samples+= len((outputs['conventions']))\n",
    "            \n",
    "            text_original.append(text_id)           \n",
    "    \n",
    "    \n",
    "    coh_correct /= total_samples\n",
    "    syntax_correct /= total_samples\n",
    "    vocab_correct /= total_samples\n",
    "    phraseology_correct /= total_samples\n",
    "    grammar_correct /= total_samples\n",
    "    conventions_correct /= total_samples\n",
    "    \n",
    "#     print(\"Accuracy Cohesion: \", coh_correct)\n",
    "#     print(\"Accuracy Syntax: \", syntax_correct)\n",
    "#     print(\"Accuracy Vocab: \", vocab_correct)\n",
    "#     print(\"Accuracy Phraselogy: \", phraseology_correct)\n",
    "#     print(\"Accuracy Grammer: \", grammar_correct)\n",
    "#     print(\"Accuracy Convetions: \", conventions_correct)\n",
    "    \n",
    "    if return_preds:\n",
    "        return text_original, coh_ans,syntax_ans,voc_ans,phr_ans,gra_ans,conv_ans\n",
    " \n",
    "        \n",
    "        \n",
    "    return coh_correct,syntax_correct,vocab_correct,phraseology_correct,grammar_correct,conventions_correct\n",
    "    \n",
    "    \n",
    "                            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testdata, batch_size=1, shuffle=False,collate_fn=collate_test_batch)\n",
    "newmodel = load_model(save_path)\n",
    "newmodel.to(device)\n",
    "all_res = test(newmodel,testloader,return_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['0000C359D63E'], ['000BAD50D026'], ['00367BB2546B']],\n",
       " [([3], 0.8307837247848511),\n",
       "  ([3], 0.6173878312110901),\n",
       "  ([3], 0.9640266299247742)],\n",
       " [([3], 0.9900375604629517),\n",
       "  ([2.5], 0.9973722696304321),\n",
       "  ([3], 0.9915764927864075)],\n",
       " [([2.5], 0.29021111130714417),\n",
       "  ([3], 0.8731154799461365),\n",
       "  ([3.5], 0.654915452003479)],\n",
       " [([3], 0.776765763759613),\n",
       "  ([2.5], 0.9945618510246277),\n",
       "  ([3], 0.7955027222633362)],\n",
       " [([3], 0.5797446966171265),\n",
       "  ([2], 0.8157117962837219),\n",
       "  ([3.5], 0.8865168690681458)],\n",
       " [([3], 0.7724350094795227),\n",
       "  ([3], 0.9311811923980713),\n",
       "  ([3], 0.9867558479309082)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame()\n",
    "test_pred['text_id'] = [item[0] for item in all_res[0]]\n",
    "test_pred['cohesion'] = [item[0][0] for item in all_res[1]]\n",
    "test_pred['syntax'] = [item[0][0] for item in all_res[2]]\n",
    "test_pred['vocabulary'] = [item[0][0] for item in all_res[3]]\n",
    "test_pred['phraseology'] = [item[0][0] for item in all_res[4]]\n",
    "test_pred['grammar'] = [item[0][0] for item in all_res[5]]\n",
    "test_pred['conventions'] = [item[0][0] for item in all_res[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtt = pd.read_csv(path+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  \\\n",
       "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0   \n",
       "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0   \n",
       "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0   \n",
       "\n",
       "   conventions  \n",
       "0          3.0  \n",
       "1          3.0  \n",
       "2          3.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  \\\n",
       "0  0000C359D63E         3     3.0         2.5          3.0      3.0   \n",
       "1  000BAD50D026         3     2.5         3.0          2.5      2.0   \n",
       "2  00367BB2546B         3     3.0         3.5          3.0      3.5   \n",
       "\n",
       "   conventions  \n",
       "0            3  \n",
       "1            3  \n",
       "2            3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mcrmse(predicted_df,gt_df,y_size=6):\n",
    "    labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "    multiplier = (1/y_size)\n",
    "    loss_sum = 0\n",
    "    for lab in labels:\n",
    "        pred_vector = predicted_df[lab].to_list()\n",
    "        gt_vector = gt_df[lab].to_list()\n",
    "        loss_sum += math.sqrt(1/len(pred_vector) * sum([(pred_vector[i]-gt_vector[i])**2 for i in range(len(pred_vector))]))\n",
    "    \n",
    "    return loss_sum*multiplier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27184929733689855"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcrmse(testtt,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
