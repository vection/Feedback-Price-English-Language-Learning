{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Prize Best score 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class GrammerDataset(Dataset):\n",
    "    pos_tag_vocab = ['CC',\n",
    "            'WRB',\n",
    "            'EX',\n",
    "            'MD',\n",
    "            'VBN',\n",
    "            'VBD',\n",
    "            'NNS',\n",
    "            'RBR',\n",
    "            'VBZ',\n",
    "            'PRP$',\n",
    "            'VB',\n",
    "            'RP',\n",
    "            'WP',\n",
    "            'VBP',\n",
    "            'JJR',\n",
    "            'VBG',\n",
    "            'PDT',\n",
    "            'JJ',\n",
    "            'JJS',\n",
    "            'WDT',\n",
    "            'IN',\n",
    "            'DT',\n",
    "            'RB',\n",
    "            'NN',\n",
    "            'PRP',\n",
    "            'TO']\n",
    "\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Dataset object for base model\n",
    "        :param data:\n",
    "        '''\n",
    "        self.text,self.text_length,self.pos_tag = self.clean_text(data['full_text'])\n",
    "        self.cohesion = np.array(self.feature_transformation(data['cohesion']))\n",
    "        self.syntax = np.array(self.feature_transformation(data['syntax']))\n",
    "        self.vocab = np.array(self.feature_transformation((data['vocabulary'])))\n",
    "        self.phraseology = np.array(self.feature_transformation(data['phraseology']))\n",
    "        self.grammer = np.array(self.feature_transformation(data['grammar']))\n",
    "        self.conventions = np.array(self.feature_transformation(data['conventions']))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def feature_transformation(self, vals):\n",
    "        self.mapping = {1:0,1.5:1,2:2,2.5:3,3:4,3.5:5,4:6,4.5:7,5:8}\n",
    "        newl = []\n",
    "        for v in vals:\n",
    "            newl.append(self.mapping[v])\n",
    "        return newl\n",
    "    def count_pos_tag(self,text):\n",
    "        tag_dict = {}\n",
    "        pos = nltk.pos_tag(text)\n",
    "        tag_types = list(set([item[1] for item in pos]))\n",
    "        for tag in tag_types:\n",
    "            tag_dict[tag] = sum([item[1]==tag for item in pos])\n",
    "        return tag_dict\n",
    "    \n",
    "\n",
    "    def clean_text(self,all_text):\n",
    "        newl = []\n",
    "        newlength = []\n",
    "        pos_tag = []\n",
    "        for text in all_text:\n",
    "            text = text.replace(\"\\n\",\" \").lower()\n",
    "            text = text.strip()\n",
    "            newl.append(text)\n",
    "            newlength.append([len(t) for t in text.split()])\n",
    "            pos_sentence = self.count_pos_tag(text.split())\n",
    "            hot_vector_pos = []\n",
    "            for po in self.pos_tag_vocab:\n",
    "                if po in pos_sentence.keys():\n",
    "                    hot_vector_pos.append(pos_sentence[po])\n",
    "                else:\n",
    "                    hot_vector_pos.append(0)\n",
    "            pos_tag.append(hot_vector_pos)\n",
    "            \n",
    "            \n",
    "        return np.array(newl),np.array(newlength),np.array(pos_tag)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx],self.text_length[idx],self.pos_tag[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "train=df.sample(frac=0.95,random_state=3) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "train_dataset = GrammerDataset(train)\n",
    "valid_dataset = GrammerDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import(\n",
    "    get_tokenizer,\n",
    "    ngrams_iterator,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "ngrams = 2\n",
    "def yield_tokens(data_iter, ngrams):\n",
    "    for text in data_iter:\n",
    "        yield iter(tokenizer(text))\n",
    "\n",
    "myvocab = build_vocab_from_iterator(yield_tokens(train_dataset.text, ngrams), specials=[\"<unk>\"])\n",
    "myvocab.set_default_index(myvocab[\"<unk>\"])\n",
    "device = 'cuda'\n",
    "def text_pipeline(x,padding=512): \n",
    "    res = myvocab(list((tokenizer(x))))\n",
    "    if len(res) < padding:\n",
    "        chars_to_add = int(padding-len(res))\n",
    "        for i in range(chars_to_add):\n",
    "            res.append(myvocab['<pad>'])\n",
    "    else:\n",
    "        res = res[:padding]\n",
    "    return res\n",
    "# self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "def collate_batch(batch):\n",
    "    text_list, cohesion, syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = [], [],[],[],[],[],[],[],[]\n",
    "    for data in batch:\n",
    "        processed_text = text_pipeline(data[0],1024)\n",
    "        text_list.append(processed_text)\n",
    "        text_length.append(len(processed_text))\n",
    "        cohesion.append(data[1])\n",
    "        syntax.append(data[2])\n",
    "        vocab.append(data[3])\n",
    "        phraseology.append(data[4])\n",
    "        grammer.append(data[5])\n",
    "        conventions.append(data[6])\n",
    "        pos_tag.append(data[8])\n",
    "        \n",
    "        \n",
    "    cohesion = torch.tensor(cohesion, dtype=torch.int64)\n",
    "    syntax = torch.tensor(syntax, dtype=torch.int64)\n",
    "    vocab = torch.tensor(vocab, dtype=torch.int64)\n",
    "    phraseology = torch.tensor(phraseology, dtype=torch.int64)\n",
    "    grammer = torch.tensor(grammer, dtype=torch.int64)\n",
    "    conventions = torch.tensor(conventions, dtype=torch.int64)\n",
    "    pos_tag_vals = torch.tensor(pos_tag, dtype=torch.int64)\n",
    "    text_length = torch.tensor(text_length, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    return text_list.to(device), cohesion.to(device), syntax.to(device),vocab.to(device), phraseology.to(device),grammer.to(device), conventions.to(device),text_length.to(device),pos_tag_vals.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self, num_scored=6):\n",
    "        super().__init__()\n",
    "        self.rmse = nn.HuberLoss()\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score += self.rmse(yhat[:, i], y[:,i]) / self.num_scored\n",
    "\n",
    "        return score\n",
    "    \n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout=0.3, pad_idx=0,\n",
    "                 fc_hidden2=64, fc_hidden1=256):\n",
    "        super().__init__()\n",
    "        self.hidden_size = fc_hidden2\n",
    "        self.input_dim = input_dim\n",
    "        self.bid = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout \n",
    "        self.fc_hidden = fc_hidden1\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # embedded to cuda if available\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "\n",
    "        #tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "        #model = DebertaModel.from_pretrained('microsoft/deberta-base', return_dict=True)\n",
    "\n",
    "#         inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "#         outputs = model(**inputs,return_dict=True)\n",
    "\n",
    "        self.num_neurons = (hidden_dim * num_layers) + 26 \n",
    "        \n",
    "        self.labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "        self.featurs_nn = []\n",
    "        self.cohesion_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        self.syntax_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        \n",
    "        self.vocabulary_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        self.phraseology_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        self.grammar_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        self.conventions_nn =  nn.Sequential(*[nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "                  nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)])\n",
    "        \n",
    "        self.features_nn = [self.cohesion_nn, self.syntax_nn, self.vocabulary_nn, self.phraseology_nn, self.grammar_nn,self.conventions_nn]\n",
    "        \n",
    "#         for lab in self.labels:\n",
    "#             fc = [nn.Linear(self.num_neurons, self.fc_hidden), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(self.fc_hidden, self.hidden_size), nn.ReLU(inplace=True),\n",
    "#                   nn.Linear(self.hidden_size, self.hidden_size), nn.ReLU(inplace=True), nn.Linear(self.hidden_size, 1)]\n",
    "#             fc = nn.Sequential(*fc)\n",
    "#             self.featurs_nn.append(fc)\n",
    "            \n",
    "        #self.featurs_nn = nn.Sequential(self.featurs_nn)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text,text_length,pos_tag):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        text_length = torch.clamp(text_length, min=1)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length.cpu(),batch_first=True,enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        res = torch.cat((hidden,pos_tag),dim=1)\n",
    "#         x = hidden.unsqueeze(0).transpose(0, 1)\n",
    "#         output = self.cnn1(x)\n",
    "#         output = self.cnn2(output)\n",
    "#         output = output.view(output.shape[0], output.shape[1] * output.shape[2])\n",
    "#         res = torch.cat((output, hidden), dim=1)\n",
    "        output_dict = {}\n",
    "        for f_nn, feature in zip(self.features_nn,self.labels):\n",
    "            newh = res.clone()\n",
    "            out = f_nn(newh)\n",
    "            output_dict[feature] = out\n",
    "        \n",
    "        last_out = torch.stack((output_dict['cohesion'],output_dict['syntax'],output_dict['vocabulary'],output_dict['phraseology'],output_dict['grammar'],output_dict['conventions']),dim=1)\n",
    "        #hidden = self.fc(hidden)\n",
    "        return last_out\n",
    "\n",
    "def save_model(model, path):\n",
    "    global myvocab\n",
    "    cfg = model.state_dict()\n",
    "    cfg['input_dim'] = model.input_dim\n",
    "    cfg['embedding_dim'] = model.embedding_dim\n",
    "    cfg['bidirectional'] = model.bid\n",
    "    cfg['hidden_dim'] = model.hidden_dim\n",
    "    cfg['num_layers'] = model.num_layers\n",
    "    cfg['output_dim'] = model.output_dim\n",
    "    cfg['dropout'] = model.dropout_rate\n",
    "    torch.save(cfg, path)\n",
    "    torch.save(myvocab,path.replace(\".pth\",\"\")+'_vocab.pth')\n",
    "    \n",
    "def load_model(path):\n",
    "    keys_to_remove = ['input_dim', 'embedding_dim', 'bidirectional', 'hidden_dim', 'num_layers', 'output_dim','dropout']\n",
    "    cfg = torch.load(path)\n",
    "    model = RNN_CNN(cfg['input_dim'],cfg['embedding_dim'],cfg['bidirectional'],cfg['hidden_dim'],cfg['num_layers'],cfg['output_dim'],cfg['dropout'])\n",
    "    \n",
    "    for key in keys_to_remove:\n",
    "        cfg.pop(key)\n",
    "    model.load_state_dict(cfg)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_vocab(path):\n",
    "    vocab = torch.load(path)\n",
    "    return vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_CNN(\n",
       "  (embedding): Embedding(21500, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (cohesion_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (syntax_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (vocabulary_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (phraseology_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (grammar_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (conventions_nn): Sequential(\n",
       "    (0): Linear(in_features=1050, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(myvocab)\n",
    "batch_size = 64\n",
    "bidirectional = True\n",
    "emb_dim = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch,drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch,drop_last=True)\n",
    "\n",
    "model = RNN(vocab_size, emb_dim,bidirectional,hidden_dim=512,num_layers=2,output_dim=1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(y_vals):\n",
    "    mapping = {0: 1, 1: 1.5, 2: 2, 3: 2.5, 4: 3, 5: 3.5, 6: 4, 7: 4.5, 8: 5}\n",
    "    if isinstance(y_vals, torch.Tensor):\n",
    "        if len(y_vals) > 0:\n",
    "            y_vals = y_vals.tolist()\n",
    "            newl = []\n",
    "            for i in y_vals:\n",
    "                newl.append(mapping[i])\n",
    "            return newl\n",
    "        return mapping[y_vals.item()]\n",
    "    elif isinstance(y_vals,int):\n",
    "         return mapping[y_vals]\n",
    "    else:\n",
    "        if len(y_vals) > 0:\n",
    "            y_vals = y_vals.tolist()\n",
    "            newl = []\n",
    "            for i in y_vals:\n",
    "                newl.append(mapping[i])\n",
    "            return newl\n",
    "def compute_acc(predicted,gt):\n",
    "    total_true = sum([predicted[i]==gt[i] for i in range(len(gt))])\n",
    "    return total_true/len(gt), total_true\n",
    "def compare_tensor_acc(preds, gt):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    proba, class_label = preds.max(dim=1)\n",
    "    predict_total = [(transform_labels(cl), pr) for cl,pr in zip(class_label.tolist(),proba.tolist())]\n",
    "    predict_only = [pt[0] for pt in predict_total]\n",
    "    gra_acc,true = compute_acc(predict_only,gt.tolist())\n",
    "    return predict_only,true\n",
    "\n",
    "import math\n",
    "def mcrmse_score(labels,y_size=6):\n",
    "    #labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "    multiplier = (1/y_size)\n",
    "    loss_sum = 0\n",
    "    i = 0\n",
    "    for item in labels:\n",
    "        pred_vector = item[0]\n",
    "        gt_vector = item[1]\n",
    "        loss_sum += math.sqrt((1/len(pred_vector)) * sum([(pred_vector[i]-gt_vector[i])**2 for i in range(len(pred_vector))]))\n",
    "    \n",
    "    return loss_sum*multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 3.8244125843048096\n",
      "New model saved! better loss average  1.003210727510781\n",
      "Average loss:  1.003210727510781\n",
      "Validation loss:  0.7136328419049581\n",
      "Epoch 1\n",
      "Loss: 0.6660494804382324\n",
      "New model saved! better loss average  0.6312715390632893\n",
      "Validation loss:  0.6571313937505087\n",
      "Epoch 2\n",
      "Loss: 0.591296374797821\n",
      "New model saved! better loss average  0.596413983353253\n",
      "Validation loss:  0.6502397060394287\n",
      "Epoch 3\n",
      "Loss: 0.6356393098831177\n",
      "New model saved! better loss average  0.5865801819439592\n",
      "Validation loss:  0.6268380880355835\n",
      "Epoch 4\n",
      "Loss: 0.7281327247619629\n",
      "Validation loss:  0.6577691634496053\n",
      "Epoch 5\n",
      "Loss: 0.552669882774353\n",
      "New model saved! better loss average  0.5551371507603546\n",
      "Validation loss:  0.6820014119148254\n",
      "Epoch 6\n",
      "Loss: 0.5448476672172546\n",
      "New model saved! better loss average  0.5364593447282396\n",
      "Validation loss:  0.6414553125699362\n",
      "Epoch 7\n",
      "Loss: 0.4563513994216919\n",
      "New model saved! better loss average  0.5238961007060676\n",
      "Validation loss:  0.6794151862462362\n",
      "Epoch 8\n",
      "Loss: 0.4753154516220093\n",
      "New model saved! better loss average  0.521858434738784\n",
      "Validation loss:  0.6635376612345377\n",
      "Epoch 9\n",
      "Loss: 0.4369124174118042\n",
      "New model saved! better loss average  0.4939576783056917\n",
      "Validation loss:  0.6297558148701986\n",
      "Epoch 10\n",
      "Loss: 0.4974603056907654\n",
      "New model saved! better loss average  0.4720713830199735\n",
      "Average loss:  0.4720713830199735\n",
      "Validation loss:  0.6997957825660706\n",
      "Epoch 11\n",
      "Loss: 0.340812087059021\n",
      "New model saved! better loss average  0.4435028079254874\n",
      "Validation loss:  0.7117007374763489\n",
      "Epoch 12\n",
      "Loss: 0.42008644342422485\n",
      "New model saved! better loss average  0.4223757679092473\n",
      "Validation loss:  0.7133956352869669\n",
      "Epoch 13\n",
      "Loss: 0.365658164024353\n",
      "New model saved! better loss average  0.4187281424629277\n",
      "Validation loss:  0.6774431069691976\n",
      "Epoch 14\n",
      "Loss: 0.3956295847892761\n",
      "New model saved! better loss average  0.40075121717206363\n",
      "Validation loss:  0.6674113869667053\n",
      "Epoch 15\n",
      "Loss: 0.42087897658348083\n",
      "New model saved! better loss average  0.3755540657660057\n",
      "Validation loss:  0.6761426329612732\n",
      "Epoch 16\n",
      "Loss: 0.3438382148742676\n",
      "New model saved! better loss average  0.3669574985216404\n",
      "Validation loss:  0.7216922044754028\n",
      "Epoch 17\n",
      "Loss: 0.39188656210899353\n",
      "New model saved! better loss average  0.35386347873457547\n",
      "Validation loss:  0.7011279463768005\n",
      "Epoch 18\n",
      "Loss: 0.2973319888114929\n",
      "New model saved! better loss average  0.3446337704000802\n",
      "Validation loss:  0.6722657481829325\n",
      "Epoch 19\n",
      "Loss: 0.3510627746582031\n",
      "New model saved! better loss average  0.33195018048944147\n",
      "Validation loss:  0.7038377126057943\n",
      "Epoch 20\n",
      "Loss: 0.3190702497959137\n",
      "New model saved! better loss average  0.32004204495199795\n",
      "Average loss:  0.32004204495199795\n",
      "Validation loss:  0.6870713432629904\n",
      "Epoch 21\n",
      "Loss: 0.2972565293312073\n",
      "Validation loss:  0.6583710511525472\n",
      "Epoch 22\n",
      "Loss: 0.3346041738986969\n",
      "New model saved! better loss average  0.3116222517243747\n",
      "Validation loss:  0.6885562340418497\n",
      "Epoch 23\n",
      "Loss: 0.35672006011009216\n",
      "New model saved! better loss average  0.3071972255049081\n",
      "Validation loss:  0.68120809396108\n",
      "Epoch 24\n",
      "Loss: 0.30518701672554016\n"
     ]
    }
   ],
   "source": [
    "loss_func = MCRMSELoss()\n",
    "lr = 0.001\n",
    "max_epochs = 300\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "best_loss = 9999\n",
    "test_score = 0.35\n",
    "save_path = 'test/best_model_rmcse_loss.pth'\n",
    "for epoch in range(max_epochs):\n",
    "    all_losses = [] \n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    coh_train_correct = 0\n",
    "    syn_train_correct = 0\n",
    "    voc_train_correct = 0\n",
    "    phr_train_correct = 0\n",
    "    gr_train_correct = 0\n",
    "    con_train_correct = 0\n",
    "    train_samples = 0\n",
    "    mcrmse_s = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        # self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "        text,cohesion,syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = batch_data\n",
    "        outputs = model(text,text_length,pos_tag)\n",
    "        outputs = torch.squeeze(outputs,dim=2)\n",
    "        outputs = outputs.to(torch.float32)\n",
    "        gt_format = torch.stack((cohesion,syntax,vocab,phraseology,grammer,conventions),dim=1)\n",
    "        gt_format = gt_format.to(torch.float32)\n",
    "#         gt_format = gt_format.reshape(batch_size,6)\n",
    "        #gt_format = gt_format.view(1,0)\n",
    "\n",
    "\n",
    "\n",
    "        # loss\n",
    "        loss = loss_func(outputs,gt_format)\n",
    "        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Loss: {}\".format(loss.item()))\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def avg(lst):\n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "#     # calculating mcrmse metric\n",
    "#     mcrmse_epoch_score = mcrmse_s/len(train_loader)\n",
    "#     if mcrmse_epoch_score < best_loss:\n",
    "#         best_loss = mcrmse_epoch_score\n",
    "#         #save_model(model,save_path)\n",
    "#         print(\"New model saved! better average RMCMSE score \", best_loss)\n",
    "    avg_loss = np.mean(all_losses)\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_model(model,save_path)\n",
    "        print(\"New model saved! better loss average \", best_loss)    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Average loss: \", avg(all_losses))\n",
    "\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(valid_loader):\n",
    "            text,cohesion,syntax,vocab,phraseology,grammer,conventions,text_length,pos_tag = batch_data\n",
    "            outputs = model(text,text_length,pos_tag)\n",
    "            outputs = torch.squeeze(outputs,dim=2)\n",
    "            outputs = outputs.to(torch.float32)\n",
    "            gt_format = torch.stack((cohesion,syntax,vocab,phraseology,grammer,conventions),dim=1)\n",
    "            gt_format = gt_format.to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "            # loss\n",
    "            loss = loss_func(outputs,gt_format)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    print(\"Validation loss: \", np.mean(valid_loss))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "class GrammerTestDataset(Dataset):\n",
    "    pos_tag_vocab = ['CC',\n",
    "            'WRB',\n",
    "            'EX',\n",
    "            'MD',\n",
    "            'VBN',\n",
    "            'VBD',\n",
    "            'NNS',\n",
    "            'RBR',\n",
    "            'VBZ',\n",
    "            'PRP$',\n",
    "            'VB',\n",
    "            'RP',\n",
    "            'WP',\n",
    "            'VBP',\n",
    "            'JJR',\n",
    "            'VBG',\n",
    "            'PDT',\n",
    "            'JJ',\n",
    "            'JJS',\n",
    "            'WDT',\n",
    "            'IN',\n",
    "            'DT',\n",
    "            'RB',\n",
    "            'NN',\n",
    "            'PRP',\n",
    "            'TO']\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Dataset object for base model\n",
    "        :param data:\n",
    "        '''\n",
    "        self.text,self.text_length,self.pos_tag = self.clean_text(data['full_text'])\n",
    "        self.text_labels = data['text_id'].to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def feature_transformation(self, vals):\n",
    "        self.mapping = {1:0,1.5:1,2:2,2.5:3,3:4,3.5:5,4:6,4.5:7,5:8}\n",
    "        newl = []\n",
    "        for v in vals:\n",
    "            newl.append(self.mapping[v])\n",
    "        return newl\n",
    "    \n",
    "    def count_pos_tag(self,text):\n",
    "        tag_dict = {}\n",
    "        pos = nltk.pos_tag(text)\n",
    "        tag_types = list(set([item[1] for item in pos]))\n",
    "        for tag in tag_types:\n",
    "            tag_dict[tag] = sum([item[1]==tag for item in pos])\n",
    "        return tag_dict\n",
    "\n",
    "    def clean_text(self,all_text):\n",
    "        newl = []\n",
    "        newlength = []\n",
    "        pos_tag = []\n",
    "        for text in all_text:\n",
    "            text = text.replace(\"\\n\",\" \").lower()\n",
    "            text = text.strip()\n",
    "            newl.append(text)\n",
    "            newlength.append([len(t) for t in text.split()])\n",
    "            pos_sentence = self.count_pos_tag(text.split())\n",
    "            hot_vector_pos = []\n",
    "            for po in self.pos_tag_vocab:\n",
    "                if po in pos_sentence.keys():\n",
    "                    hot_vector_pos.append(pos_sentence[po])\n",
    "                else:\n",
    "                    hot_vector_pos.append(0)\n",
    "            pos_tag.append(hot_vector_pos)\n",
    "        return np.array(newl),np.array(newlength),np.array(pos_tag)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx],self.text_length[idx],self.pos_tag[idx],self.text_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(path+\"test.csv\")\n",
    "testdata = GrammerTestDataset(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "def collate_test_batch(batch):\n",
    "    text_list,text_length,pos_tag,text_id = [], [],[],[]\n",
    "    for data in batch:\n",
    "        processed_text = text_pipeline(data[0],512)\n",
    "        text_list.append(processed_text)\n",
    "        text_length.append(len(processed_text))\n",
    "        pos_tag.append(data[2])\n",
    "        text_id.append(data[3])\n",
    "        \n",
    "        \n",
    "        \n",
    "    text_length = torch.tensor(text_length, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    pos_tag = torch.tensor(pos_tag, dtype=torch.int64)\n",
    "    return text_list.to(device), text_length.to(device),pos_tag.to(device),text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model,loader,return_preds=False):\n",
    "    coh_correct = 0\n",
    "    syntax_correct = 0\n",
    "    vocab_correct = 0\n",
    "    phraseology_correct = 0\n",
    "    grammar_correct = 0\n",
    "    conventions_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    coh_preds = []\n",
    "    syntax_preds = []\n",
    "    voc_preds = []\n",
    "    phr_preds = []\n",
    "    gra_preds = []\n",
    "    conv_preds = []\n",
    "    text_original = []\n",
    "    \n",
    "    final_results = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(loader):\n",
    "            # self.text[idx], self.cohesion[idx],self.syntax[idx],self.vocab[idx],self.phraseology[idx],self.grammer[idx],self.conventions[idx]\n",
    "            text = batch_data[0]\n",
    "            text_length = batch_data[1]\n",
    "            pos_tag = batch_data[2]\n",
    "            text_id = batch_data[3]\n",
    "            outputs = model(text,text_length,pos_tag)\n",
    "            \n",
    "            outputs = outputs.tolist()\n",
    "            for item,t in zip(outputs,text_id):\n",
    "                coh_preds.append(item[0][0])\n",
    "                syntax_preds.append(item[1][0])\n",
    "                voc_preds.append(item[2][0])\n",
    "                phr_preds.append(item[3][0])\n",
    "                gra_preds.append(item[4][0])\n",
    "                conv_preds.append(item[5][0])   \n",
    "                text_original.append(t)\n",
    "            \n",
    "            labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "            \n",
    "            \n",
    "           \n",
    "    \n",
    "    \n",
    "    final_results['text_id'] = text_original\n",
    "    final_results['cohesion'] = coh_preds\n",
    "    final_results['syntax'] = syntax_preds\n",
    "    final_results['vocabulary'] = voc_preds\n",
    "    final_results['phraseology'] = phr_preds\n",
    "    final_results['grammar'] = gra_preds\n",
    "    final_results['conventions'] = conv_preds\n",
    "    \n",
    "    if return_preds:\n",
    "        return final_results\n",
    " \n",
    "        \n",
    "        \n",
    "    return coh_correct,syntax_correct,vocab_correct,phraseology_correct,grammar_correct,conventions_correct\n",
    "    \n",
    "    \n",
    "                            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testdata, batch_size=3, shuffle=False,collate_fn=collate_test_batch)\n",
    "newmodel = load_model(save_path)\n",
    "newmodel.to(device)\n",
    "all_res = test(newmodel,testloader,return_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>078201045481</td>\n",
       "      <td>2.531787</td>\n",
       "      <td>2.239652</td>\n",
       "      <td>2.661514</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.112138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>079FDB42E429</td>\n",
       "      <td>4.388169</td>\n",
       "      <td>4.000633</td>\n",
       "      <td>4.386132</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.014640</td>\n",
       "      <td>0.114995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07A4DAFDE5B7</td>\n",
       "      <td>4.602616</td>\n",
       "      <td>4.203226</td>\n",
       "      <td>4.657094</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.110394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07C6BB6ADA38</td>\n",
       "      <td>4.361373</td>\n",
       "      <td>3.972078</td>\n",
       "      <td>4.376444</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>0.111441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07CE77EA56C5</td>\n",
       "      <td>5.728912</td>\n",
       "      <td>5.167780</td>\n",
       "      <td>5.509420</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>0.107097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0EC7D67618F4</td>\n",
       "      <td>4.151865</td>\n",
       "      <td>3.810284</td>\n",
       "      <td>4.248954</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.011698</td>\n",
       "      <td>0.110117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0ED9E0B4AC30</td>\n",
       "      <td>3.856188</td>\n",
       "      <td>3.538497</td>\n",
       "      <td>3.952511</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.114750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0EE67777ABB5</td>\n",
       "      <td>3.969946</td>\n",
       "      <td>3.617338</td>\n",
       "      <td>4.018198</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>0.114321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0EEE49F99224</td>\n",
       "      <td>3.909279</td>\n",
       "      <td>3.573855</td>\n",
       "      <td>3.980236</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.112401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0F19DD8FEEDC</td>\n",
       "      <td>5.587925</td>\n",
       "      <td>5.052044</td>\n",
       "      <td>5.417179</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.112665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0   078201045481  2.531787  2.239652    2.661514     0.016086  0.012376   \n",
       "1   079FDB42E429  4.388169  4.000633    4.386132     0.008437  0.014640   \n",
       "2   07A4DAFDE5B7  4.602616  4.203226    4.657094     0.007949  0.011250   \n",
       "3   07C6BB6ADA38  4.361373  3.972078    4.376444     0.007702  0.012865   \n",
       "4   07CE77EA56C5  5.728912  5.167780    5.509420     0.009224  0.010871   \n",
       "..           ...       ...       ...         ...          ...       ...   \n",
       "95  0EC7D67618F4  4.151865  3.810284    4.248954     0.004842  0.011698   \n",
       "96  0ED9E0B4AC30  3.856188  3.538497    3.952511     0.006845  0.015726   \n",
       "97  0EE67777ABB5  3.969946  3.617338    4.018198     0.006614  0.012759   \n",
       "98  0EEE49F99224  3.909279  3.573855    3.980236     0.005911  0.012232   \n",
       "99  0F19DD8FEEDC  5.587925  5.052044    5.417179     0.010858  0.010503   \n",
       "\n",
       "    conventions  \n",
       "0      0.112138  \n",
       "1      0.114995  \n",
       "2      0.110394  \n",
       "3      0.111441  \n",
       "4      0.107097  \n",
       "..          ...  \n",
       "95     0.110117  \n",
       "96     0.114750  \n",
       "97     0.114321  \n",
       "98     0.112401  \n",
       "99     0.112665  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mcrmse(predicted_df,gt_df,y_size=6):\n",
    "    labels = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
    "    multiplier = (1/y_size)\n",
    "    loss_sum = 0\n",
    "    for lab in labels:\n",
    "        pred_vector = predicted_df[lab].to_list()\n",
    "        gt_vector = gt_df[lab].to_list()\n",
    "        loss_sum += math.sqrt(1/len(pred_vector) * sum([(pred_vector[i]-gt_vector[i])**2 for i in range(len(pred_vector))]))\n",
    "    \n",
    "    return loss_sum*multiplier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.046327735099217"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcrmse(all_res,testtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
